#!/bin/bash
#PBS -A csc205
#PBS -N G500
#PBS -j oe
#PBS -q batch
#PBS -l walltime=00:30:00
#PBS -l nodes=128
#PBS -l gres=atlas1%atlas2
#PBS -V

# TOY - 26
# MINI - 29
# SMALL - 32
# MEDIUM - 36
# LARGE - 39
# HUGE - 42

# export GASNET_MAX_SEGSIZE='512MB'
cd $PBS_O_WORKDIR
export LD_LIBRARY_PATH=/opt/gcc/4.9.0/snos/lib64:$LD_LIBRARY_PATH

export LD_LIBRARY_PATH=$PROJ_DIR/hclib/modules/openshmem/lib:$LD_LIBRARY_PATH
export HCLIB_LOCALITY_FILE=$PROJ_DIR/hclib/locality_graphs/titan.no_gpu.json
export GASNET_BACKTRACE=1
# export SMA_SYMMETRIC_SIZE=1073741824
# export GASNET_PHYSMEM_MAX=1073741824
export SMA_SYMMETRIC_SIZE=4294967296
export GASNET_PHYSMEM_MAX=4294967296
export LD_PRELOAD=/lustre/atlas/sw/tbb/43/sles11.3_gnu4.8.2/source/build/linux_intel64_gcc_cc4.8.2_libc2.11.3_kernel3.0.101_release/libtbbmalloc_proxy.so.2
ulimit -c unlimited

export HIPER_TRACE_DIR=$PROJWORK/csc205/hiper-traces/

NODES=$PBS_NUM_NODES

# for NODES in 32 64 128 256; do
for NODES in $PBS_NUM_NODES; do
    echo Running with NODES=$NODES

export GRAPH_SIZE=30

# export SMA_SYMMETRIC_SIZE=1073741824
# export GASNET_PHYSMEM_MAX=$SMA_SYMMETRIC_SIZE
# export OMP_NUM_THREADS=16
# aprun -n $(($NODES)) -d $OMP_NUM_THREADS -N 1 -cc 0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15 ../mpi/graph500_mpi_simple $GRAPH_SIZE 16
# export OMP_NUM_THREADS=1
# aprun -n $(($NODES * 16)) -d $OMP_NUM_THREADS -N 16 -cc 0:1:2:3:4:5:6:7:8:9:10:11:12:13:14:15 ../mpi/graph500_mpi_simple $GRAPH_SIZE 16

# for THREADS_PER_PE in 1 16; do
#     export OMP_NUM_THREADS=$THREADS_PER_PE
#     PES_PER_NODE=$((16 / $THREADS_PER_PE))
#     TOTAL_PES=$(($PES_PER_NODE * $NODES))
#     CC=$(python generate_cc.py $PES_PER_NODE)
# 
#     PARAMS="-n $TOTAL_PES -d $THREADS_PER_PE -N $PES_PER_NODE -cc $CC"
#     echo $THREADS_PER_PE threads per pe, $PES_PER_NODE PEs per node, $TOTAL_PES PEs in total, $PARAMS
# 
#     export SMA_SYMMETRIC_SIZE=1073741824
#     export GASNET_PHYSMEM_MAX=$SMA_SYMMETRIC_SIZE
#     aprun $PARAMS ../mpi/graph500_mpi_replicated $GRAPH_SIZE 16
# 
# # #     TUNED_EXE=../mpi-tuned-2d/mpi/graph500_mpi_custom_${TOTAL_PES}
# #     if [[ -f $TUNED_EXE ]]; then
# #         export SMA_SYMMETRIC_SIZE=1073741824
# #         export GASNET_PHYSMEM_MAX=$SMA_SYMMETRIC_SIZE
# #         export MPICH_MAX_THREAD_SAFETY=multiple
# #         aprun $PARAMS $TUNED_EXE $GRAPH_SIZE
# #     fi
# done

export SMA_SYMMETRIC_SIZE=1073741824
export GASNET_PHYSMEM_MAX=$SMA_SYMMETRIC_SIZE
export OMP_NUM_THREADS=1
export HCLIB_WORKERS=$OMP_NUM_THREADS
aprun -n $(($NODES * 16)) -d $OMP_NUM_THREADS -N 16 -cc $(python generate_cc.py 1) ./bfs_oshmem-single-mailbox-concurrent $GRAPH_SIZE 16

export SMA_SYMMETRIC_SIZE=1073741824
export GASNET_PHYSMEM_MAX=$SMA_SYMMETRIC_SIZE
export OMP_NUM_THREADS=1
export HCLIB_WORKERS=$OMP_NUM_THREADS
aprun -n $(($NODES * 16)) -d $OMP_NUM_THREADS -N 16 -cc $(python generate_cc.py 1) ./bfs_oshmem-single-mailbox-hiper $GRAPH_SIZE 16

export SMA_SYMMETRIC_SIZE=1073741824
# export SMA_SYMMETRIC_SIZE=1178599424
export GASNET_PHYSMEM_MAX=$SMA_SYMMETRIC_SIZE
export OMP_NUM_THREADS=1
export HCLIB_WORKERS=$OMP_NUM_THREADS
aprun -n $(($NODES * 16)) -d $OMP_NUM_THREADS -N 16 -cc $(python generate_cc.py 1) ./bfs_oshmem-single-mailbox-concurrent-crc $GRAPH_SIZE 16

export SMA_SYMMETRIC_SIZE=1073741824
export GASNET_PHYSMEM_MAX=$SMA_SYMMETRIC_SIZE
export OMP_NUM_THREADS=1
export HCLIB_WORKERS=$OMP_NUM_THREADS
aprun -n $(($NODES * 16)) -d $OMP_NUM_THREADS -N 16 -cc $(python generate_cc.py 1) ./bfs_oshmem-single-mailbox-concurrent-crc-hiper $GRAPH_SIZE 16

# export SMA_SYMMETRIC_SIZE=1073741824
# export GASNET_PHYSMEM_MAX=$SMA_SYMMETRIC_SIZE
# export OMP_NUM_THREADS=1
# export HCLIB_WORKERS=$OMP_NUM_THREADS
# aprun -n $(($NODES * 16)) -d $OMP_NUM_THREADS -N 16 -cc $(python generate_cc.py 1) ./bfs_oshmem-bitvector $GRAPH_SIZE 16

done
