\chapter{Running QMCPACK}
QMCPACK uses MPI/OpenMP hybrid method for the parallelization. The optimal choice of MPI nodes and OpenMP threads depends on
\begin{itemize}
\item{} how much memory one needs to store read-only one-body orbitals
\item{} memory hierarchy.
\end{itemize}

For the common multicore chips of today, using one MPI per node and setting \icode{OMP\_NUM\_THREADS} to the number of cores of a node will work. But, if a walker can be fit into a NUMA node, it is better to use one MPI per NUMA node. For instance, on Cray XT5 with dual hex-core AMD chips, setting \icode{OMP\_NUM\_THREADS=6} will work best. [check this]

Setting the number of walkers and samples can be tricky with parallel runs. The basic rule is at least one walker per thread.  See Setting samples to learn how to control QMC runs.

%what's a good number of... threads, processors, walkers etc.
\section{Direct execution}
\section{Through a job queue}

discuss portable batch system
\section{Output and data analysis tools}
discuss HDF5 format in more detail

mention other external tools in the repository
\section{Considerations for accurate and efficient calculations}
\subsection{Timestep studies}
\subsection{Monitoring fixed-node error}
\subsection{Correcting pseudopotential errors}
\subsection{Finite size corrections}
\subsection{Cross-validation with other QMC codes}
\section{Chasing down problems}

%  Output and Data analysis tools
%  Considerations for accurate and efficient calculations
%    Timestep Studies
%    Monitoring Fixed Node Error
%    Correcting Pseudopotential Errors
%    Finite Size Corrections
%      Correcting for momentum quantization 
%        k point sampling
%      Correcting for image interactions
%        The S(k) correction
%        Model Periodic Coulomb 
%  Chasing down problems
%    Common Errors and Gotchas
%    Warnings and Error messages
%    Where to go for help

