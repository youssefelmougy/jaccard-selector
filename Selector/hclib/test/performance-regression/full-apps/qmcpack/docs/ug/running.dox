/*! \page runningP Running QMCPACK

To run QMCPACK using 8 threads:
\code
export OMP_NUM_THREADS=8
qmcpack/build/bin/qmcapp input.xml
\endcode

To use <c>mpirun</c> with 4 MPI tasks and 8 threads:
\code
export OMP_NUM_THREADS=8
mpirun -np 4 qmcpack/build/bin/qmcapp input.xml
\endcode

\section mpiopenmpS Parallelism using MPI/OpenMP programming model

QMCPACK uses MPI/OpenMP hybrid method for the parallelization.  QMC simulations
can always gain by using more parallel processing units and therefore MPI and
OpenMP parallelization is enabled by default, unless the programming
environment does not support MPI or OpenMP.

The optimal choice of MPI nodes and OpenMP threads depends on
- how much memory one needs to store read-only one-body orbitals
- memory hierarchy.

For the common multicore chips of today, using one MPI per node and setting
<c>OMP_NUM_THREADS</c> to the number of cores of a node will work. But, if a
walker can be fit into a NUMA node, it is better to use one MPI per NUMA node.
For instance, on Cray XC30 with dual octa-core Intel chips, setting
<c>OMP_NUM_THREADS=8</c> will work best.

Setting the number of walkers and samples can be tricky with parallel runs. The
basic rule is at least one walker per thread.  See Setting samples to learn how
to control QMC runs.

\section multiruns Running multiple QMC instances

One can submit/run a job which combines multiple QMC simulations. This is useful
when a simulation involves scanning a parameter space, configuration space and
and k-point intergration.

In order to run multiple instances, 
\code
[mpi options] qmcapp input.list
\endcode
The name "input.list" can be anything but cannot have xml extension and contains a list 
of QMCPACK XML inputfiles. The input file to run four k-points simulatenously has
\code
input-tw0.xml
input-tw1.xml
input-tw2.xml
input-tw3.xml
\endcode
Each xml file is a complete input file with <c>particleset</c>,
<c>wavefunction</c>, <c>hamiltonian</c> and <c>qmc</c> sections.

Any combination of input files can be used but one should consider the
load-balance issues. The slowest QMC problem will set the run time. 

When a job is launched, 
- The number of input XML files is used to split MPI communicators into groups.
- Each MPI group is an independent unit and processes the input xml file..
- The names, e.g., psi0, ion0, can be identical or distinct: a MPI has a separate name space.
- The output files has an addition field, <c>g###</c> denoting the MPI group for the input (QMC instance).

\section restartS Restarting from a previous QMC run

To restart a QMC run, these two steps have to be taken.

First, set <c>qmc/\@checkpoint</c> to the number of blocks between the
checkpoint. The default is  <c>qmc/\@checkpoint="-1"</c> and no data will be
stored. With any positive integer, e.g.,
\code
<qmc method="dmc" checkpoint="10">
....
</qmc>
\endcode
three files are generated at the moment of checkpoint:
- <c>TITLE.s###.config.h5</c> :configuration of all the walkers 
- <c>TITLE.s###.qmc.xml</c>  : state of the active QMC driver
- <c>TITLE.s###.random.xml</c>  : state of random number generators

Second, the main QMCPACK input file should contain, a <c>mcwalkerset</c> node prior to any
<c>qmc</c> sections. E.g.,
\code
<mcwalkerset fileroot="TITLE.s###" node="-1" nprocs="2" version="0 6" collected="yes"/>
<qmc method="dmc">
.....
</qmc>
\endcode

When a QMCPACK run is completed, <c>*.cont.xml</c> is written by the
application.  It should have <c>mcwalkerset</c> node based on the project ID
and sequence number according to the <c>qmc</c> blocks that have been executed.

Make sure to remove any unnecessary <c>qmc</c> sections or parameters, e.g.
<c>warmupsteps</c> has to be removed, unless a new warm-up block have to be
executed.

The configuration is "gathered" to the root node of a MPI group and the
attributes such as <c>mcwalkerset/\@node</c> and <c>mcwalkerset/\@nproces</c>
are mostly to guide a run but will be overwritten at the run time.

If continuing random-number generators is critical, it is important to use the
same number of parallel processing units, i.e. MPI tasks x OpenMp threads.  If
they are not the same between runs, new random generators will be used for the
restart run.

*/
